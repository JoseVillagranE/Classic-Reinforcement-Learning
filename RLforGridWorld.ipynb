{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv():\n",
    "    \n",
    "    def __init__(self, gridsize=4, startState='00', terminalStates=['33'], ditches=['12'],\n",
    "                ditchPenalty=-10, turnPenalty=-1, winReward=100, mode='prod'):\n",
    "        \n",
    "        self.mode=mode\n",
    "        self.gridSize=min(gridsize, 9)\n",
    "        self.create_stateSpace()\n",
    "        self.actionSpace = [0, 1, 2, 3]\n",
    "        self.actionDict = {0: 'UP', 1:'DOWN', 2:'LEFT', 3:'RIGHT'}\n",
    "        self.startState = startState\n",
    "        self.terminalStates = terminalStates\n",
    "        self.ditches = ditches\n",
    "        self.winReward = winReward\n",
    "        self.ditchPenalty = ditchPenalty\n",
    "        self.turnPenalty = turnPenalty\n",
    "        self.stateCount = self.get_stateSpace_len()\n",
    "        self.actionCount = self.get_actionSpace_len()\n",
    "        self.stateDict = {k: v for k, v in zip(self.stateSpace, range(self.stateCount))}\n",
    "        self.currentState = self.startState\n",
    "        \n",
    "        if self.mode == 'debug':\n",
    "            print(\"State Space\", self.stateSpace)\n",
    "            print(\"State Dict\", self.stateDict)\n",
    "            print(\"Action Space\", self.actionSpace)\n",
    "            print(\"Action Dict\", self.actionDict)\n",
    "            print(\"Start State\", self.startState)\n",
    "            print(\"Terminal States\", self.terminalStates)\n",
    "            print(\"Ditches\", self.ditches)\n",
    "            print(\"WinReward:{}, TurnPenalty:{}, DitchPenalty:{}\".format(self.winReward, self.turnPenalty, self.ditchPenalty))\n",
    "        \n",
    "    def create_stateSpace(self):\n",
    "        \n",
    "        self.stateSpace = []\n",
    "        for row in range(self.gridSize):\n",
    "            for col in range(self.gridSize):\n",
    "                self.stateSpace.append(str(row)+ str(col))\n",
    "                \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "    \n",
    "    def get_stateSpace(self):\n",
    "        return self.stateSpace\n",
    "    \n",
    "    def get_actionSpace(self):\n",
    "        return self.actionSpace\n",
    "    \n",
    "    def get_actionDict(self):\n",
    "        return self.actionDict\n",
    "    \n",
    "    def get_stateSpace_len(self):\n",
    "        return len(self.stateSpace)\n",
    "    \n",
    "    def get_actionSpace_len(self):\n",
    "        return len(self.actionSpace)\n",
    "    \n",
    "    def next_state(self, current_state, action):\n",
    "        s_row = int(current_state[0])\n",
    "        s_col = int(current_state[1])\n",
    "        next_row = s_row\n",
    "        next_col = s_col\n",
    "        \n",
    "        if action == 0: next_row = max(0, s_row - 1)\n",
    "        if action == 1: next_row = min(self.gridSize-1, s_row+1)\n",
    "        if action == 2: next_col = max(0, s_col - 1)\n",
    "        if action == 3: next_col = min(self.gridSize - 1, s_col + 1)\n",
    "            \n",
    "        new_state = str(next_row) + str(next_col)\n",
    "        if new_state in self.stateSpace:\n",
    "            if new_state in self.terminalStates: self.isGameEnd = True\n",
    "            if self.mode == 'debug':\n",
    "                print(\"CurrentState:{}, Action:{}, NextState:{}\".format(current_state, action, new_state))\n",
    "            return new_state\n",
    "        else:\n",
    "            return current_state\n",
    "    \n",
    "    def compute_reward(self, state):\n",
    "        \n",
    "        reward = 0\n",
    "        reward += self.turnPenalty\n",
    "        if state in self.ditches: reward += self.ditchPenalty\n",
    "        if state in self.terminalStates: reward += self.winReward\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.isGameEnd = False\n",
    "        self.totalAccumulatedReward = 0\n",
    "        self.totalTurns = 0\n",
    "        self.currentState = self.startState\n",
    "        return self.currentState\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        if self.isGameEnd:\n",
    "            raise(\"Game is Over Exception\")\n",
    "        if action not in self.actionSpace:\n",
    "            raise(\"Invalid Action Exception\")\n",
    "        self.currentState = self.next_state(self.currentState, action)\n",
    "        obs = self.currentState\n",
    "        reward = self.compute_reward(obs)\n",
    "        done = self.isGameEnd\n",
    "        if self.mode=='debug':\n",
    "            print(\"Obs:{}, Reward:{}, Done:{}, TotalTurns:{}\".format(obs, reward, done, self.totalTurns))\n",
    "        return obs, reward, done, self.totalTurns\n",
    "    \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \n",
    "    def __init__(self, env=GridWorldEnv(), discountingFactor = 0.9, convergenceThreshold=1e-4, iterationThreshold=30,\n",
    "                 mode='prod'):\n",
    "        self.env = env\n",
    "        self.gamma = discountingFactor\n",
    "        self.th = convergenceThreshold\n",
    "        self.maxIter = iterationThreshold\n",
    "        self.stateCount = self.env.get_stateSpace_len()\n",
    "        self.actionCount = self.env.get_actionSpace_len()\n",
    "        self.uniformActionProbability = 1.0/self.actionCount\n",
    "        self.stateDict = self.env.stateDict\n",
    "        self.actionDict = self.env.actionDict\n",
    "        self.mode = mode\n",
    "        self.V = np.zeros(self.stateCount)\n",
    "        self.Q = [np.zeros(self.actionCount) for s in range(self.stateCount)]\n",
    "        self.Policy = np.zeros(self.stateCount)\n",
    "        self.totalReward = 0\n",
    "        self.totalStep = 0\n",
    "        \n",
    "    def reset_episode(self):\n",
    "        \n",
    "        self.totalReward = 0\n",
    "        self.totalSteps = 0\n",
    "        \n",
    "    def iterate_value(self):\n",
    "        \n",
    "        self.V = np.zeros(self.stateCount)\n",
    "        for i in range(self.maxIter):\n",
    "            last_V = np.copy(self.V)\n",
    "            for state_index in range(self.stateCount):\n",
    "                current_state = self.env.stateSpace[state_index]\n",
    "                for action in self.env.actionSpace:\n",
    "                    next_state = self.env.next_state(current_state, action)\n",
    "                    reward = self.env.compute_reward(next_state)\n",
    "                    next_state_index = self.env.stateDict[next_state]\n",
    "                    self.Q[state_index][action] = reward + self.gamma*last_V[next_state_index]\n",
    "                    \n",
    "                if self.mode == 'debug':\n",
    "                    print(\"Q(s={}):{}\".format(current_state, self.Q[state_index]))\n",
    "                self.V[state_index] = max(self.Q[state_index])\n",
    "                if np.sum(np.fabs(last_V - self.V)) <= self.th:\n",
    "                    print(\"Convergence Achieved in {}th iteration. Breaking V_Iteration loop!\".format(i))\n",
    "                    break\n",
    "    \n",
    "    def extract_optimal_policy(self):\n",
    "        \n",
    "        self.Policy = np.argmax(self.Q, axis=1)\n",
    "        if self.mode=='debug':\n",
    "            print(\"Optimal Policy:\", self.Policy)\n",
    "            \n",
    "    def run_episode(self):\n",
    "        \n",
    "        self.reset_episode()\n",
    "        obs = self.env.reset()\n",
    "        while True:\n",
    "            action = self.Policy[self.env.stateDict[obs]]\n",
    "            new_obs, reward, done, _ = self.env.step(action)\n",
    "            if self.mode == 'debug':\n",
    "                print(\"PrevObs:{}, Obs:{}, Reward:{}, Done:{}\".format(obs, new_obs, reward, done))\n",
    "            self.totalReward += reward\n",
    "            self.totalSteps += 1\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                obs = new_obs\n",
    "        return self.totalReward\n",
    "    \n",
    "    def evaluate_policy(self, n_episodes=100):\n",
    "        \n",
    "        episode_scores = []\n",
    "        if self.mode =='debug':print(\"Running {} episodes!\".format(n_episodes))\n",
    "        for e, episode in enumerate(range(n_episodes)):\n",
    "            score = self.run_episode()\n",
    "            episode_scores.append(score)\n",
    "            if self.mode=='debug': print(\"Score in {} episode = {}\".format(e, score))\n",
    "        return np.mean(episode_scores)\n",
    "    \n",
    "    def solve_mdp(self, n_episode=100):\n",
    "        \n",
    "        if self.mode=='debug':\n",
    "            print(\"Iteration Values..\")\n",
    "        self.iterate_value()\n",
    "        if self.mode=='debug':\n",
    "            print(\"Extracting Optimal Policy..\")\n",
    "        self.extract_optimal_policy()\n",
    "        if self.mode == 'debug':\n",
    "            print(\"Scoring Policy..\")\n",
    "        return self.evaluate_policy(n_episode)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(mode='prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueIteration = ValueIteration(env, mode='prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation Score =  94.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy Evaluation Score = \", valueIteration.solve_mdp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
